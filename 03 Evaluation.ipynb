{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from visualisationMetrics import *\n",
    "from dataLoader import *\n",
    "from utils.utils import to_pkl, from_pkl, js_divergence, fid\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NSETS = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train's shape is (4328, 1, 1, 2), X_test's shape is (481, 1, 1, 2)\n",
      "y_train's label shape is (4328, 1, 1, 43), y_test's label shape is (481, 1, 1, 43)\n",
      "(481, 1, 43)\n",
      "(481, 1, 2)\n",
      "(481, 43, 1)\n"
     ]
    }
   ],
   "source": [
    "real_data = load_dataset(data_mode='Train')\n",
    "real_dataloader = data.DataLoader(real_data, batch_size=1, num_workers=1, shuffle=True)\n",
    "\n",
    "real_paths = []\n",
    "real_conds = []\n",
    "\n",
    "for i, (cond, sim) in enumerate(real_dataloader):\n",
    "    sim = sim.cpu().detach().numpy()\n",
    "    sim = sim.reshape(sim.shape[1], sim.shape[3])\n",
    "    real_paths.append(sim)\n",
    "    cond = cond.cpu().detach().numpy()\n",
    "    cond = cond.reshape(cond.shape[1], cond.shape[3])\n",
    "    real_conds.append(cond)\n",
    "\n",
    "real_paths = np.array(real_paths)\n",
    "real_conds = np.array(real_conds)\n",
    "print(real_paths.shape)\n",
    "print(real_conds.shape)\n",
    "\n",
    "real_vals = np.transpose(real_paths, (0,2,1))\n",
    "print(real_vals.shape)\n",
    "to_pkl('results/real_vals.pkl', real_vals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LoadSynthetic import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syn_vals(model_path='./logs/latest/Model/checkpoint', n=10 **kwargs):\n",
    "    data = Synthetic_Dataset(model_path=model_path, n=n, dataset=real_data, seq_len=real_data.output_size, conditions_dim=real_data.X_train.shape[-1], **kwargs)\n",
    "    dataloader = data.DataLoader(syn_data, batch_size=1, num_workers=1, shuffle=True, **kwargs)\n",
    "\n",
    "    paths = []\n",
    "    conds = []\n",
    "\n",
    "    for i, (cond, sim) in enumerate(dataloader):\n",
    "        sim = sim.cpu().detach().numpy()\n",
    "        sim = sim.reshape(sim.shape[1], sim.shape[3])\n",
    "        paths.append(sim)\n",
    "        cond = cond.cpu().detach().numpy()\n",
    "        cond = cond.reshape(cond.shape[1], cond.shape[3])\n",
    "        conds.append(cond)\n",
    "        \n",
    "    paths = np.array(paths)\n",
    "    conds = np.array(conds)\n",
    "    vals = np.transpose(paths, (0,2,1))\n",
    "    np.random.shuffle(vals)\n",
    "    return vals\n",
    "\n",
    "def get_syn_val_sets(nsets, **kwargs):\n",
    "    val_sets = []\n",
    "    for i in range(nsets):\n",
    "        val_sets.append(get_syn_vals(**kwargs))\n",
    "    return val_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_val_sets = get_syn_val_sets(nsets=NSETS)\n",
    "to_pkl('results/syn_val_sets.pkl', syn_val_sets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load benchmark data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import GBM_Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gbm_vals(n=10, **kwargs):\n",
    "    simulator = GBM_Simulator(dataset=real_data, nsamples=n)\n",
    "    paths = simulator.run()\n",
    "    paths = np.reshape(paths, (-1,1,paths.shape[-1]))\n",
    "\n",
    "    vals = np.transpose(paths, (0, 2, 1))\n",
    "    np.random.shuffle(vals)\n",
    "    return vals\n",
    "\n",
    "def get_gbm_val_sets(nsets, **kwargs):\n",
    "    val_sets = []\n",
    "    for i in range(nsets):\n",
    "        val_sets.append(get_syn_vals(**kwargs))\n",
    "    return val_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_val_sets = get_gbm_val_sets(nsets=NSETS)\n",
    "to_pkl('results/gbm_val_sets.pkl', gbm_val_sets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_vals = from_pkl('results/real_vals.pkl')\n",
    "syn_val_sets = from_pkl('results/syn_val_sets.pkl')\n",
    "gbm_val_sets = from_pkl('results/gbm_val_sets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scores = {\n",
    "    'js_pca': {\n",
    "        'syn': [],\n",
    "        'gbm': []\n",
    "    },\n",
    "    'js_tsne': {\n",
    "        'syn': [],\n",
    "        'gbm': []\n",
    "    },\n",
    "    'fid': {\n",
    "        'syn': [],\n",
    "        'gbm': []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def js_div(real_vals, syn_vals, gbm_vals, mode, n_components=10, **kwargs):\n",
    "    real, syn, gbm = dim_reduction([real_vals, syn_vals, gbm_vals], n_components=n_components, mode=mode, **kwargs)\n",
    "    syn_score = js_divergence(real, syn, verbose=False)\n",
    "    gbm_score = js_divergence(real, gbm, verbose=False)\n",
    "    return syn_score, gbm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 125.23it/s]\n"
     ]
    }
   ],
   "source": [
    "for syn_vals, gbm_vals in tqdm(zip(syn_val_sets, gbm_val_sets)):\n",
    "    # for iter in range(3):\n",
    "    #     # JS-PCA\n",
    "    #     syn_score, gbm_score = js_div(real_vals, syn_vals, gbm_vals, mode='pca')\n",
    "    #     eval_scores['js_pca']['syn'].append(syn_score)\n",
    "    #     eval_scores['js_pca']['gbm'].append(gbm_score)\n",
    "        \n",
    "    #     # JS-TSNE\n",
    "    #     syn_score, gbm_score = js_div(real_vals, syn_vals, gbm_vals, mode='tsne')\n",
    "    #     eval_scores['js_tsne']['syn'].append(syn_score)\n",
    "    #     eval_scores['js_tsne']['gbm'].append(gbm_score)\n",
    "    \n",
    "    # FID\n",
    "    syn_score = fid(real_vals, syn_vals)\n",
    "    gbm_score = fid(real_vals, gbm_vals)\n",
    "    eval_scores['fid']['syn'].append(syn_score)\n",
    "    eval_scores['fid']['gbm'].append(gbm_score)\n",
    "\n",
    "to_pkl('results/eval_scores.pkl', eval_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run hypothesis tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "eval_scores = from_pkl('results/eval_scores.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis_test(syn_scores, gbm_scores):\n",
    "    syn_scores = np.array(syn_scores)\n",
    "    gbm_scores = np.array(gbm_scores)\n",
    "    print(\"Synthetic:\")\n",
    "    print(f\"\\tmean  = {syn_scores.mean()}\")\n",
    "    print(f\"\\tstdev = {syn_scores.std()}\")\n",
    "    print(\"Benchmark:\")\n",
    "    print(f\"\\tmean  = {gbm_scores.mean()}\")\n",
    "    print(f\"\\tstdev = {gbm_scores.std()}\")    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA JS-Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic:\n",
      "\tmean  = 3.4993658878660705\n",
      "\tstdev = 0.36186398609631226\n",
      "Benchmark:\n",
      "\tmean  = 3.435675627903045\n",
      "\tstdev = 0.42417521597528157\n"
     ]
    }
   ],
   "source": [
    "hypothesis_test(eval_scores['js_pca']['syn'], eval_scores['js_pca']['gbm'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE JS-Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic:\n",
      "\tmean  = 0.23291941152119952\n",
      "\tstdev = 0.025523383725958342\n",
      "Benchmark:\n",
      "\tmean  = 0.23105095644953846\n",
      "\tstdev = 0.030810723779002624\n"
     ]
    }
   ],
   "source": [
    "hypothesis_test(eval_scores['js_tsne']['syn'], eval_scores['js_tsne']['gbm'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FID Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic:\n",
      "\tmean  = 0.012538626986795778\n",
      "\tstdev = 0.0003862124438919217\n",
      "Benchmark:\n",
      "\tmean  = 0.012547210414578019\n",
      "\tstdev = 0.0003672441023192968\n"
     ]
    }
   ],
   "source": [
    "hypothesis_test(eval_scores['fid']['syn'], eval_scores['fid']['gbm'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
